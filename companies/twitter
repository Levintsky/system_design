# http://highscalability.com/blog/2016/4/20/how-twitter-handles-3000-images-per-second.html

### How Twitter Handles 3,000 Images Per Second
# 3,000 (200 GB) images per second.
# in 2015 Twitter was able to save $6 million

# Doing the simplest thing that can possibly work can really screw you.
# Decouple (media from tweets)
# Move handles not blobs.
# Experiment and research. 20 day TTL (time to live), save 4T per day
# On demand. Old image variants could be deleted because they could be recreated on the fly rather than precomputed.
# Progressive JPEG

# Twitter in 2012
Write
 >> Problem #1: A Lot of Wasted Network Bandwidth
 >> Problem #2: Doesn't Scale Well for New Larger Media Sizes
 >> Problem #3: Inefficient Use of Internal Bandwidth
 >> Problem #4: Bloated Storage Footprint
Read
 >> Problem #5: Impossible to Introduce New Variants

# Twitter in 2016
Write
 >> Decoupling media upload from tweeting.
 >> Segmented resumable uploads.
Read
 >> Introduced a CDN Origin Server called MinaBird.
Client Improvements (Android)

# http://highscalability.com/blog/2011/12/19/how-twitter-stores-250-million-tweets-a-day-using-mysql.html
### How Twitter Stores 250 Million Tweets A Day Using MySQL
 >> One of the interesting stories he told was of the transition from Twitter's old way of storing tweets using temporal sharding, 
 >> to a more distributed approach using a new tweet store called T-bird, which is built on top of Gizzard, which is built using MySQL.

 
# Twitter's Original Tweet Store:
 >> Temporally sharded tweets was a good-idea-at-the-time architecture. Temporal sharding simply means tweets from the same date range are stored together on the same shard.
 >> The problem is tweets filled up one machine, then a second, and then a third. You end up filling up one machine after another.
 >> Load balancing. Most of the old machines didn't get any traffic because people are interested in what is happening now, especially with Twitter.
 >> Expensive. They filled up one machine, with all its replication slaves, every three weeks, which is an expensive setup.
 >> Logistically complicated. Building a whole new cluster every three weeks is a pain for the DBA team. 

# Twitter's New Tweet Store:
 >> When you tweet it's stored in an internal system called T-bird, which is built on top of Gizzard. Secondary indexes are stored in a separate system called T-flock, which is also Gizzard based.
 >> Unique IDs for each tweet are generated by Snowflake, which can be more evenly sharded across a cluster. FlockDB is used for ID to ID mapping, storing the relationships between IDs (uses Gizzard).
 >> Gizzard is Twitter's distributed data storage framework built on top of MySQL (InnoDB).

# MySQL Isn't Used For Everything:
 >> Cassandra is used for high velocity writes, and lower velocity reads.
 >> Hadoop is used to process unstructured and large datasets, hundreds of billions of rows.
 >> Vertica is being used for analytics and large aggregations and joins so they don't have to write MapReduce jobs.   
