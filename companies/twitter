# 2/17/2016
# http://blog.gainlo.co/index.php/2016/02/17/system-design-interview-question-how-to-design-twitter-part-1/
 Define problem: MVP
  1. Data Modeling
  2. Server Feeds
 Infinite scroll: recent top N
 Detect Fake users:
 How to rank feeds:
  C (Affinity) * P(erformance) * T(ype) * R(ecency)
 How to measure ranking:
 @feature:
  Include uid in the tweets
 Retweet:

# 2/24/2016
# http://blog.gainlo.co/index.php/2016/02/24/system-design-interview-question-how-to-design-twitter-part-2/
 # Trending Topics:
  1. Get trending topics:
    most frequent hashtags in last N hours
  2. rank
 # Who to follow:
  1. within 2 - 3 steps;
  2. Accounts with most followers;
 # How to scale?
 # How to evaluate?
 # Moments: (more complated than trending topics)
  1. Categorize each tweet/topic: news, sports, etc;
   > pre-define, supervised learning;
   > clustering
  2. Generate and ranking trending topics at current moment C;
  3. Generate and rank for each topic;
 # Search
  index, rank, retrieval

# http://highscalability.com/blog/2016/4/20/how-twitter-handles-3000-images-per-second.html

### How Twitter Handles 3,000 Images Per Second
# 3,000 (200 GB) images per second.
# in 2015 Twitter was able to save $6 million

# Doing the simplest thing that can possibly work can really screw you.
# Decouple (media from tweets)
# Move handles not blobs.
# Experiment and research. 20 day TTL (time to live), save 4T per day
# On demand. Old image variants could be deleted because they could be recreated on the fly rather than precomputed.
# Progressive JPEG

# Twitter in 2012
Write
 >> Problem #1: A Lot of Wasted Network Bandwidth
 >> Problem #2: Doesn't Scale Well for New Larger Media Sizes
 >> Problem #3: Inefficient Use of Internal Bandwidth
 >> Problem #4: Bloated Storage Footprint
Read
 >> Problem #5: Impossible to Introduce New Variants

# Twitter in 2016
Write
 >> Decoupling media upload from tweeting.
 >> Segmented resumable uploads.
Read
 >> Introduced a CDN Origin Server called MinaBird.
Client Improvements (Android)

# http://highscalability.com/blog/2011/12/19/how-twitter-stores-250-million-tweets-a-day-using-mysql.html
### How Twitter Stores 250 Million Tweets A Day Using MySQL
 >> One of the interesting stories he told was of the transition from Twitter's old way of storing tweets using temporal sharding, 
 >> to a more distributed approach using a new tweet store called T-bird, which is built on top of Gizzard, which is built using MySQL.

 
# Twitter's Original Tweet Store:
 >> Temporally sharded tweets was a good-idea-at-the-time architecture. Temporal sharding simply means tweets from the same date range are stored together on the same shard.
 >> The problem is tweets filled up one machine, then a second, and then a third. You end up filling up one machine after another.
 >> Load balancing. Most of the old machines didn't get any traffic because people are interested in what is happening now, especially with Twitter.
 >> Expensive. They filled up one machine, with all its replication slaves, every three weeks, which is an expensive setup.
 >> Logistically complicated. Building a whole new cluster every three weeks is a pain for the DBA team. 

# Twitter's New Tweet Store:
 >> When you tweet it's stored in an internal system called T-bird, which is built on top of Gizzard. Secondary indexes are stored in a separate system called T-flock, which is also Gizzard based.
 >> Unique IDs for each tweet are generated by Snowflake, which can be more evenly sharded across a cluster. FlockDB is used for ID to ID mapping, storing the relationships between IDs (uses Gizzard).
 >> Gizzard is Twitter's distributed data storage framework built on top of MySQL (InnoDB).

# MySQL Isn't Used For Everything:
 >> Cassandra is used for high velocity writes, and lower velocity reads.
 >> Hadoop is used to process unstructured and large datasets, hundreds of billions of rows.
 >> Vertica is being used for analytics and large aggregations and joins so they don't have to write MapReduce jobs.  


### https://www.hiredintech.com/classrooms/system-design/lesson/68
Designing Twitter
# Asking Questions:
how many users? 10m user, 100m http requests
how connected? each user will be following 200 other users on average, outliers with tens of thousands
new tweets and favor, how many requests? 10 m requests, each twitter favored twice

what is expected?
10 million tweets per day
20 million tweet favorites per day
2 billion "follow" relations
Some users and tweets could generate an extraordinary amount of traffic

posting new tweets
following a user
favoriting a tweet
displaying data about users and tweets

Relational Database: MySQL?
Cache in front of data-center (Much slower to read from the disk than from the memory)

# Database Schema
Table users
 ID (id)
 username (username)
 full name (first_name & last_name)
 password related fields like hash and salt (password_hash & password_salt)
 date of creation and last update (created_at & updated_at)
 description (description)
 and maybe some other fields...

Table tweets
 ID (id)
 content (content)
 date of creation (created_at)
 user ID of author (user_id)

These two entities have several types of relations between them:
 users create tweets
 users can follow users
 users favorite tweets

Table connections
 ID of user that follows (follower_id)
 ID of user that is followed (followee_id)
 date of creation (created_at)

Table favorites
 ID of user that favorited (user_id)
 ID of favorited tweet (tweet_id)
 date of creation (created_at)

# Building a RESTful API
how our front-end would “talk” to the back-end system
GET /api/users/<username>
GET /api/users/<username>/tweets
GET /api/users/<username>/tweets?page=4

GET /api/users/<username>/followers
GET /api/users/<username>/followees

POST /api/users/<username>/tweets
POST /api/users/<username>/followers

# Increased number of read requests
data grows too quickly
sharding

# Unexpected Traffic
outliers will generate peaks in the number of requests that our application receives

### http://highscalability.com/blog/2011/12/19/how-twitter-stores-250-million-tweets-a-day-using-mysql.html
How Twitter Stores 250 Million Tweets A Day Using MySQL
https://github.com/twitter/mysql

